"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[6854],{65097:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"NeoEdge NG4500 Series/Application Guide/DINOv3","title":"DINOv3","description":"---","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/1-NeoEdge NG4500 Series/3-Application Guide/4-DINOv3.md","sourceDirName":"1-NeoEdge NG4500 Series/3-Application Guide","slug":"/NeoEdge NG4500 Series/Application Guide/DINOv3","permalink":"/wiki-documents/en/docs/NeoEdge NG4500 Series/Application Guide/DINOv3","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Object Detection","permalink":"/wiki-documents/en/docs/NeoEdge NG4500 Series/Application Guide/Object Detection"},"next":{"title":"Nx Meta Platfrom","permalink":"/wiki-documents/en/docs/NeoEdge NG4500 Series/Application Guide/Nx Meta"}}');var i=r(74848),s=r(28453);const a={},o="DINOv3",d={},l=[{value:"1. Overview",id:"1-overview",level:2},{value:"2. Environment Preparation",id:"2-environment-preparation",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Software Requirements",id:"software-requirements",level:3},{value:"3. DINOv3 Patch Feature Heatmap Visualization Example",id:"3-dinov3-patch-feature-heatmap-visualization-example",level:2},{value:"3.1 System Environment Setup",id:"31-system-environment-setup",level:3},{value:"3.2 Obtain DINOv3 Source Code",id:"32-obtain-dinov3-source-code",level:3},{value:"3.3 Create Heatmap Visualization Test Code",id:"33-create-heatmap-visualization-test-code",level:3},{value:"3.4 Download Pretrained Models",id:"34-download-pretrained-models",level:3},{value:"3.5 Testing",id:"35-testing",level:3},{value:"4. DINOv3 + KMeans Unsupervised Video Stream Segmentation Example",id:"4-dinov3--kmeans-unsupervised-video-stream-segmentation-example",level:2},{value:"4.1 Principle Overview",id:"41-principle-overview",level:3},{value:"4.2 Environment Preparation",id:"42-environment-preparation",level:3},{value:"4.3 Obtain DINOv3 Source Code",id:"43-obtain-dinov3-source-code",level:3},{value:"4.4 Create Video Stream Unsupervised Segmentation Test Code",id:"44-create-video-stream-unsupervised-segmentation-test-code",level:3},{value:"4.5 Download Pretrained Models",id:"45-download-pretrained-models",level:3},{value:"4.6 Testing",id:"46-testing",level:3},{value:"5.\xa0 References",id:"5-references",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"dinov3",children:"DINOv3"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"This article provides a detailed guide on deploying Meta\u2019s DINOv3 foundation vision model (released in 2025) on the NVIDIA Jetson Orin platform. It covers model introduction, system requirements, heatmap visualization, and two core application cases including unsupervised video stream segmentation, helping developers get started quickly and understand DINOv3\u2019s visual representation capabilities."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"1-overview",children:"1. Overview"}),"\n",(0,i.jsx)(n.p,{children:"DINOv3 is a Vision Transformer (ViT) model based on self-supervised learning and Gram anchoring technology. Compared to previous models such as DINOv2 and MoCo, it offers significant improvements in feature extraction generalization, dense feature quality, and spatial structure understanding. Key parameters and advantages include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Parameter Size:"}),"\xa0Available in multiple configurations such as 7B (7 billion parameters) to suit different computational resources."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Training Data:"}),"\xa0Trained on an ultra-large-scale dataset of 1.7 billion images (e.g., LVD-1689M), covering a wide variety of scenes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Core Technology:"}),"\xa0Gram anchoring mechanism enhances inter-feature relationship modeling, addressing the weak local feature issue in traditional ViTs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Functional Features:"}),"\xa0Supports multi-task (classification, segmentation, detection), multi-resolution (flexible input sizes), and dense feature extraction (per-patch feature output)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Advantages:"}),"\xa0Outperforms on classification and segmentation tasks across benchmarks such as ImageNet and COCO."]}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Method"}),(0,i.jsx)(n.th,{children:"Advantages"}),(0,i.jsx)(n.th,{children:"Limitations"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"MoCo/SimCLR"}),(0,i.jsx)(n.td,{children:"Simple and efficient, good for instance discrimination"}),(0,i.jsx)(n.td,{children:"Weak local features"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"iBOT/BEiT/MAE"}),(0,i.jsx)(n.td,{children:"Strong at inpainting & dense features"}),(0,i.jsx)(n.td,{children:"Lacks global consistency"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"DINOv2"}),(0,i.jsx)(n.td,{children:"Self-distillation + contrastive learning, strong performance"}),(0,i.jsx)(n.td,{children:"Dense feature degradation in large models (e.g., ViT-L)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"DINOv3"}),(0,i.jsx)(n.td,{children:"Gram anchoring, large scale, strong dense features"}),(0,i.jsx)(n.td,{children:"Very high training resource requirements"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"2-environment-preparation",children:"2. Environment Preparation"}),"\n",(0,i.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Component"}),(0,i.jsx)(n.th,{children:"Requirement"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Device"}),(0,i.jsx)(n.td,{children:"Jetson Orin (Nano / NX / AGX)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Memory"}),(0,i.jsx)(n.td,{children:"\u2265 8GB (larger models require more)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Storage"}),(0,i.jsx)(n.td,{children:"\u2265 64GB (depends on model size)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPU"}),(0,i.jsx)(n.td,{children:"NVIDIA GPU with CUDA support"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"software-requirements",children:"Software Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ubuntu 20.04 / 22.04 (JetPack 5.1.1+ recommended)"}),"\n",(0,i.jsx)(n.li,{children:"NVIDIA CUDA toolkit and drivers (included in JetPack)"}),"\n",(0,i.jsx)(n.li,{children:"Docker (optional, for containerized deployment)"}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["\u2699\ufe0f \u4f7f\u7528 ",(0,i.jsx)(n.code,{children:"jetson_clocks"})," \u548c\u68c0\u67e5 ",(0,i.jsx)(n.code,{children:"nvpmodel"}),"\uff0c\u542f\u7528\u6700\u5927\u6027\u80fd\u6a21\u5f0f\u4ee5\u83b7\u5f97\u6700\u4f73\u63a8\u7406\u6548\u679c\u3002"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"3-dinov3-patch-feature-heatmap-visualization-example",children:"3. DINOv3 Patch Feature Heatmap Visualization Example"}),"\n",(0,i.jsx)(n.p,{children:"This example extracts patch features from an image using DINOv3, computes the cosine similarity between a target patch and all other patches, and generates a heatmap to visualize the model\u2019s understanding of spatial structure (e.g., object part associations, contours)."}),"\n",(0,i.jsx)(n.h3,{id:"31-system-environment-setup",children:"3.1 System Environment Setup"}),"\n",(0,i.jsx)(n.p,{children:"Install JetPack SDK and core Python dependencies to ensure compatibility:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'sudo apt update\r\nsudo apt install python3-pip git -y\r\npip3 install --upgrade pip\r\npip3 install pillow transformers accelerate modelscope opencv-python tqdm addict simplejson sortedcontainers\r\npip install numpy==1.24.4\r\npip install "pyarrow==12.0.1"\r\npip install "modelscope[datasets]"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"32-obtain-dinov3-source-code",children:"3.2 Obtain DINOv3 Source Code"}),"\n",(0,i.jsx)(n.p,{children:"Clone the official Facebook Research repository to get the model framework code:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/facebookresearch/dinov3.git\r\ncd dinov3\n"})}),"\n",(0,i.jsx)(n.h3,{id:"33-create-heatmap-visualization-test-code",children:"3.3 Create Heatmap Visualization Test Code"}),"\n",(0,i.jsxs)(n.p,{children:["Create a\xa0",(0,i.jsx)(n.code,{children:"test_dinov3"}),"\xa0directory and a\xa0",(0,i.jsx)(n.code,{children:"dinov3_vision_test.py"}),"\xa0file as follows:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create working directory\r\nmkdir test_dinov3\r\ncd test_dinov3\r\n\r\n# Edit test code\r\nvim dinov3_vision_test.py\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Sample code:"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Note: To switch between different DINOv3 models (ViT Small/Base/Large, ConvNeXT), refer to the table below to modify the import and instantiation code. Ensure the weight file matches the model type, otherwise errors will occur. (For more model definitions, see\xa0",(0,i.jsx)(n.code,{children:"dinov3/models/vision_transformer.py"}),")"]}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model Type"}),(0,i.jsx)(n.th,{children:"Import Statement"}),(0,i.jsx)(n.th,{children:"Instantiation Example"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ViT Small"}),(0,i.jsx)(n.td,{children:"from dinov3.models.vision_transformer import vit_small"}),(0,i.jsx)(n.td,{children:"model = vit_small(patch_size=16)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ViT Base"}),(0,i.jsx)(n.td,{children:"from dinov3.models.vision_transformer import vit_base"}),(0,i.jsx)(n.td,{children:"model = vit_base(patch_size=16)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ViT Huge"}),(0,i.jsx)(n.td,{children:"from dinov3.models.vision_transformer import vit_huge2"}),(0,i.jsx)(n.td,{children:"model = vit_huge2(patch_size=16)"})]})]})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torchvision.transforms as transforms\r\nimport torch.nn.functional as F\r\nimport matplotlib.pyplot as plt\r\nfrom PIL import Image\r\nimport argparse\r\nimport os\r\n\r\nfrom dinov3.models.vision_transformer import vit_base\r\n\r\ndef compute_patch_similarity_heatmap(patch_features, H, W, target_patch_coord):\r\n    target_idx = target_patch_coord[0] * W + target_patch_coord[1]\r\n    target_feature = patch_features[0, target_idx]\r\n    similarities = F.cosine_similarity(\r\n        target_feature.unsqueeze(0),\r\n        patch_features[0],\r\n        dim=1\r\n    )\r\n    heatmap = similarities.reshape(H, W).cpu().numpy()\r\n    return heatmap\r\n\r\ndef plot_similarity_heatmap(heatmap, target_patch_coord, save_path=None):\r\n    H, W = heatmap.shape\r\n    fig, ax = plt.subplots(figsize=(10, 8))\r\n    im = ax.imshow(heatmap, cmap='viridis', aspect='equal')\r\n    ax.plot(target_patch_coord[1], target_patch_coord[0], 'ro', markersize=10)\r\n    plt.colorbar(im, ax=ax, label='Cosine Similarity') \r\n    ax.set_xlabel('Width (patch index)')\r\n    ax.set_ylabel('Height (patch index)')\r\n    ax.set_title(f'Cosine Similarity to Patch at {target_patch_coord}')\r\n    plt.tight_layout()\r\n    if save_path:\r\n        plt.savefig(save_path)\r\n        print(f\"Heatmap saved to {save_path}\")\r\n    else:\r\n        plt.show()\r\n    plt.close(fig)\r\n    return fig, ax\r\n\r\ndef main(image_path, model_path, patch_size=16, input_size=224, output_path=\"patch_similarity_heatmap.png\"):\r\n    image = Image.open(image_path).convert(\"RGB\")\r\n    transform = transforms.Compose([\r\n        transforms.Resize((input_size, input_size)),\r\n        transforms.ToTensor(),\r\n    ])\r\n    tensor_image = transform(image).unsqueeze(0)  # (1, 3, H, W)\r\n\r\n    model = vit_base(patch_size=patch_size)\r\n    state_dict = torch.load(model_path, map_location=\"cpu\")\r\n    model.load_state_dict(state_dict, strict=False)\r\n    model.eval()\r\n    if torch.cuda.is_available():\r\n        model.cuda()\r\n        tensor_image = tensor_image.cuda()\r\n\r\n    with torch.no_grad():\r\n        features_dict = model.forward_features(tensor_image)\r\n        patch_features = features_dict[\"x_norm_patchtokens\"]\r\n        num_patches = patch_features.shape[1]\r\n        H = W = int(num_patches ** 0.5)\r\n        target_patch_coord = (H // 2, W // 2)\r\n        heatmap = compute_patch_similarity_heatmap(patch_features, H, W, target_patch_coord)\r\n        plot_similarity_heatmap(heatmap, target_patch_coord, save_path=output_path)\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--image', type=str, required=True, help='Path to input image')\r\n    parser.add_argument('--model', type=str, required=True, help='Path to DINOv3 .pth model')\r\n    parser.add_argument('--patch_size', type=int, default=16, help='Patch size (default=16)')\r\n    parser.add_argument('--input_size', type=int, default=224, help='Input image size (default=224)')\r\n    parser.add_argument('--output', type=str, default='patch_similarity_heatmap.png', help='Output heatmap image path')\r\n    args = parser.parse_args()\r\n    main(args.image, args.model, args.patch_size, args.input_size, args.output)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"34-download-pretrained-models",children:"3.4 Download Pretrained Models"}),"\n",(0,i.jsx)(n.p,{children:"DINOv3 offers various architectures (ViT Small/Base/Large, ConvNeXT) and training sets (LVD-1689M, SAT-493M). Pretrained weights can be found on Hugging Face. Common models include:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model Name (Hugging Face)"}),(0,i.jsx)(n.th,{children:"Parameters"}),(0,i.jsx)(n.th,{children:"Hugging Face Repo"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vits16-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"21M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vits16-pretrain-lvd1689m",children:"facebook/dinov3-vits16-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vits16plus-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"29M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vits16plus-pretrain-lvd1689m",children:"facebook/dinov3-vits16plus-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vitb16-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"86M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m",children:"facebook/dinov3-vitb16-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vitl16-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"300M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vitl16-pretrain-lvd1689m",children:"facebook/dinov3-vitl16-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vith16plus-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"840M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vith16plus-pretrain-lvd1689m",children:"facebook/dinov3-vith16plus-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vit7b16-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"6,716M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vit7b16-pretrain-lvd1689m",children:"facebook/dinov3-vit7b16-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vitl16-pretrain-sat493m"}),(0,i.jsx)(n.td,{children:"300M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vitl16-pretrain-sat493m",children:"facebook/dinov3-vitl16-pretrain-sat493m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-vit7b16-pretrain-sat493m"}),(0,i.jsx)(n.td,{children:"6,716M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-vit7b16-pretrain-sat493m",children:"facebook/dinov3-vit7b16-pretrain-sat493m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-convnext-tiny-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"29M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-convnext-tiny-pretrain-lvd1689m",children:"facebook/dinov3-convnext-tiny-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-convnext-small-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"50M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-convnext-small-pretrain-lvd1689m",children:"facebook/dinov3-convnext-small-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-convnext-base-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"89M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-convnext-base-pretrain-lvd1689m",children:"facebook/dinov3-convnext-base-pretrain-lvd1689m"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dinov3-convnext-large-pretrain-lvd1689m"}),(0,i.jsx)(n.td,{children:"198M"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook/dinov3-convnext-large-pretrain-lvd1689m",children:"facebook/dinov3-convnext-large-pretrain-lvd1689m"})})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"35-testing",children:"3.5 Testing"}),"\n",(0,i.jsx)(n.p,{children:"Run the following command to generate a heatmap for a specified image (using ViT Base as an example):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"python dinov_vision_test.py --image test.png --model dinov3_vits16_pretrain_lvd1689m.pth --output patch_similarity_heatmap.png\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--image"}),": Path to the input image."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--model"}),": Selected DINOv3 model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"--output"}),": Output path for the heatmap image."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example output:"})}),"\n",(0,i.jsxs)(n.p,{children:["The generated\xa0",(0,i.jsx)(n.code,{children:"patch_similarity_heatmap.png"}),"\xa0visually reflects the similarity between the selected patch and others, showcasing DINOv3\u2019s understanding of spatial structure."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:r(76251).A+"",width:"900",height:"1040"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"4-dinov3--kmeans-unsupervised-video-stream-segmentation-example",children:"4. DINOv3 + KMeans Unsupervised Video Stream Segmentation Example"}),"\n",(0,i.jsx)(n.p,{children:"This example combines DINOv3\u2019s patch feature extraction with KMeans clustering to achieve real-time, unsupervised video segmentation on Jetson Orin NX + imx219 camera, enabling foreground/background separation in video frames without manual labeling."}),"\n",(0,i.jsx)(n.h3,{id:"41-principle-overview",children:"4.1 Principle Overview"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"DINOv3:"}),"\xa0A self-supervised Vision Transformer (ViT) model capable of learning rich visual structure features."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"KMeans Clustering:"}),"\xa0Performs unsupervised clustering of patch token features for each frame, assigning each patch to a cluster for region segmentation."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Workflow:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"IMX219 camera captures video frames"}),"\n",(0,i.jsx)(n.li,{children:"DINOv3 extracts patch features"}),"\n",(0,i.jsx)(n.li,{children:"KMeans clusters the features"}),"\n",(0,i.jsx)(n.li,{children:"Clustering results are visualized and overlaid on the original image"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"42-environment-preparation",children:"4.2 Environment Preparation"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"System dependencies:"})}),"\n",(0,i.jsx)(n.p,{children:"Install required libraries for Scikit-learn clustering and PyTorch inference:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"pip install scikit-learn Pillow\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Hardware setup:"})}),"\n",(0,i.jsx)(n.p,{children:"Connect the IMX219 camera to the device (ensure the metal contacts on the cable face up)."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:r(84721).A+"",width:"1197",height:"636"})}),"\n",(0,i.jsx)(n.h3,{id:"43-obtain-dinov3-source-code",children:"4.3 Obtain DINOv3 Source Code"}),"\n",(0,i.jsx)(n.p,{children:"Same as section 3.2, clone the dinov3 repository and enter the project directory."}),"\n",(0,i.jsx)(n.h3,{id:"44-create-video-stream-unsupervised-segmentation-test-code",children:"4.4 Create Video Stream Unsupervised Segmentation Test Code"}),"\n",(0,i.jsxs)(n.p,{children:["Create a\xa0",(0,i.jsx)(n.code,{children:"test_dinov3"}),"\xa0directory and a\xa0",(0,i.jsx)(n.code,{children:"dinov3_kmeans_test.py"}),"\xa0file, and write the following code:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create working directory\r\nmkdir test_dinov3\r\ncd test_dinov3\r\n\r\n# Edit test code\r\nvim dinov3_kmeans_test.py\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Sample code:"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Note: To switch between different DINOv3 models (ViT Small/Base/Large, ConvNeXT), refer to the table below to modify the import and instantiation code. Ensure the weight file matches the model type, otherwise errors will occur. (For more model definitions, see\xa0",(0,i.jsx)(n.code,{children:"dinov3/models/vision_transformer.py"}),")"]}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model Type"}),(0,i.jsx)(n.th,{children:"Import Statement"}),(0,i.jsx)(n.th,{children:"Instantiation Example"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ViT Small"}),(0,i.jsx)(n.td,{children:"from dinov3.models.vision_transformer import vit_small"}),(0,i.jsx)(n.td,{children:"model = vit_small(patch_size=16)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ViT Base"}),(0,i.jsx)(n.td,{children:"from dinov3.models.vision_transformer import vit_base"}),(0,i.jsx)(n.td,{children:"model = vit_base(patch_size=16)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ViT Huge"}),(0,i.jsx)(n.td,{children:"from dinov3.models.vision_transformer import vit_huge2"}),(0,i.jsx)(n.td,{children:"model = vit_huge2(patch_size=16)"})]})]})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nimport numpy as np\r\nfrom sklearn.cluster import KMeans\r\nimport time\r\n\r\nfrom dinov3.models.vision_transformer import vit_base\r\n\r\ndef preprocess_frame(frame, input_size):\r\n    # OpenCV BGR to RGB\r\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n    pil_img = transforms.ToPILImage()(frame_rgb)\r\n    transform = transforms.Compose([\r\n        transforms.Resize((input_size, input_size)),\r\n        transforms.ToTensor(),\r\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r\n    ])\r\n    tensor_img = transform(pil_img).unsqueeze(0)\r\n    return tensor_img\r\n\r\ndef segment_with_kmeans(patch_features, H, W, n_clusters=3):\r\n    # patch_features: [1, num_patches, dim]\r\n    features = patch_features[0].cpu().numpy()  # [num_patches, dim]\r\n    kmeans = KMeans(n_clusters=n_clusters, n_init=1, max_iter=50, random_state=0)\r\n    labels = kmeans.fit_predict(features)\r\n    mask = labels.reshape(H, W)\r\n    return mask\r\n\r\ndef mask_to_color(mask, n_clusters):\r\n    mask_norm = np.uint8(255 * mask / (n_clusters - 1))\r\n    mask_color = cv2.applyColorMap(mask_norm, cv2.COLORMAP_JET)\r\n    return mask_color\r\n\r\ndef main(model_path, patch_size=16, input_size=224, n_clusters=3):\r\n    model = vit_base(patch_size=patch_size)\r\n    state_dict = torch.load(model_path, map_location="cpu")\r\n    model.load_state_dict(state_dict, strict=False)\r\n    model.eval()\r\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n    model.to(device)\r\n\r\n    gst_pipeline = (\r\n        "nvarguscamerasrc ! "\r\n        "video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)NV12, framerate=(fraction)30/1 ! "\r\n        "nvvidconv ! "\r\n        "video/x-raw, format=(string)BGRx ! "\r\n        "videoconvert ! "\r\n        "video/x-raw, format=(string)BGR ! "\r\n        "appsink"\r\n    )\r\n    cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)\r\n    if not cap.isOpened():\r\n        print("Error: Camera not opened")\r\n        return\r\n\r\n    print("Camera opened successfully! Press \'q\' to quit.")\r\n    while True:\r\n        t0 = time.time()\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            print("Can\'t receive frame. Exiting ...")\r\n            break\r\n\r\n        tensor_frame = preprocess_frame(frame, input_size)\r\n        tensor_frame = tensor_frame.to(device)\r\n\r\n        with torch.no_grad():\r\n            features_dict = model.forward_features(tensor_frame)\r\n            patch_features = features_dict["x_norm_patchtokens"]  # [1, num_patches, dim]\r\n            num_patches = patch_features.shape[1]\r\n            H = W = int(num_patches ** 0.5)\r\n\r\n        mask = segment_with_kmeans(patch_features, H, W, n_clusters=n_clusters)\r\n\r\n        mask_color = mask_to_color(mask, n_clusters)\r\n        mask_resized = cv2.resize(mask_color, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\r\n\r\n        overlay = cv2.addWeighted(frame, 0.6, mask_resized, 0.4, 0)\r\n\r\n        fps = 1.0 / (time.time() - t0)\r\n        cv2.putText(overlay, f"FPS: {fps:.2f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\r\n\r\n        cv2.imshow(\'DINOv3 + KMeans Unsupervised Segmentation\', overlay)\r\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n\r\nif __name__ == \'__main__\':\r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--model\', type=str, required=True, help=\'Path to DINOv3 .pth model\')\r\n    parser.add_argument(\'--patch_size\', type=int, default=16, help=\'Patch size (default=16)\')\r\n    parser.add_argument(\'--input_size\', type=int, default=224, help=\'Input image size (default=224)\')\r\n    parser.add_argument(\'--n_clusters\', type=int, default=3, help=\'Number of clusters for KMeans\')\r\n    args = parser.parse_args()\r\n    main(args.model, args.patch_size, args.input_size, args.n_clusters)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"45-download-pretrained-models",children:"4.5 Download Pretrained Models"}),"\n",(0,i.jsx)(n.p,{children:"For the KMeans segmentation example, use the pretrained weights for ViT Base/Large models (same as the heatmap example). Download links are in section 3.4."}),"\n",(0,i.jsx)(n.h3,{id:"46-testing",children:"4.6 Testing"}),"\n",(0,i.jsx)(n.p,{children:"Run the following command to start real-time video segmentation (example: ViT Base model, 2 clusters):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python test-dinov3-kmeans.py \\\r\n  --model dinov3_vitb16_pretrain.pth \\\r\n  --patch_size 16 \\\r\n  --input_size 224 \\\r\n  --n_clusters 2\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"--model"}),": Path to the DINOv3 weight file, must match the model architecture (e.g., use vitb16 weights for vit_base)."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"--patch_size"}),": Patch size, usually 16."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"--input_size"}),": Input image resize size, usually 224."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"--n_clusters"}),": Number of KMeans clusters; 2 for foreground/background, 3+ for more regions."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example result:"}),(0,i.jsx)(n.br,{}),"\n","The system can perform unsupervised recognition of objects in the video, segmenting real-time video into foreground and background (or more regions) without manual labeling."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:r(64219).A+"",width:"637",height:"377"})}),"\n",(0,i.jsx)(n.h2,{id:"5-references",children:"5.\xa0 References"}),"\n",(0,i.jsxs)(n.p,{children:["\u25cf\xa0",(0,i.jsx)(n.a,{href:"https://github.com/facebookresearch/dinov3",children:"DINOv3 Official Repository"})]}),"\n",(0,i.jsxs)(n.p,{children:["\u25cf\xa0",(0,i.jsx)(n.a,{href:"https://huggingface.co/facebook",children:"Hugging Face DINOv3 Model Hub"})]}),"\n",(0,i.jsxs)(n.p,{children:["\u25cf\xa0",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/embedded/jetson",children:"NVIDIA Jetson Official Documentation"})]}),"\n",(0,i.jsxs)(n.p,{children:["\u25cf\xa0",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/",children:"TensorRT Documentation \u2014 NVIDIA TensorRT Documentation"})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},64219:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/DINOv3_KMeans-10dd5360a5a7ec5fda82cb0bdd664aca.gif"},76251:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/DINOv3_heatmap-8aff8856ad1559df736e70e6258e140b.png"},84721:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/NG45XX_IMX219-028db30ad99023127ece3d626bb10ebc.png"},28453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(96540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);