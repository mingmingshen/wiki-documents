"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[389],{75540:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>d,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"NeoEdge NG4500 Series/Application Guide/Deepseek-r1","title":"Chatbot","description":"---","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/1-NeoEdge NG4500 Series/3-Application Guide/1-Deepseek-r1.md","sourceDirName":"1-NeoEdge NG4500 Series/3-Application Guide","slug":"/NeoEdge NG4500 Series/Application Guide/Deepseek-r1","permalink":"/wiki-documents/en/docs/NeoEdge NG4500 Series/Application Guide/Deepseek-r1","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"jupyter","permalink":"/wiki-documents/en/docs/NeoEdge NG4500 Series/NG4500-CB01 Development Board/Software Guide/Software Frameworks and Tools/jupyter"},"next":{"title":"Pose Estimation","permalink":"/wiki-documents/en/docs/NeoEdge NG4500 Series/Application Guide/mediapipe"}}');var l=i(74848),s=i(28453);const d={},t="Chatbot",c={},o=[{value:"1. Overview",id:"1-overview",level:2},{value:"2. Environment Setup",id:"2-environment-setup",level:2},{value:"Hardware",id:"hardware",level:3},{value:"Software",id:"software",level:3},{value:"3. Install Ollama (Inference Engine)",id:"3-install-ollama-inference-engine",level:2},{value:"Method A: Native Script Installation",id:"method-a-native-script-installation",level:3},{value:"Method B: Docker Deployment",id:"method-b-docker-deployment",level:3},{value:"Verify Ollama Running",id:"verify-ollama-running",level:3},{value:"4. Run DeepSeek-R1 Model",id:"4-run-deepseek-r1-model",level:2},{value:"Launch Model",id:"launch-model",level:3},{value:"Model Version Comparison",id:"model-version-comparison",level:3},{value:"5. Web Interface (Open WebUI)",id:"5-web-interface-open-webui",level:2},{value:"Install Open WebUI (Docker)",id:"install-open-webui-docker",level:3},{value:"Access WebUI",id:"access-webui",level:3},{value:"6. Performance Optimization",id:"6-performance-optimization",level:2},{value:"7. Troubleshooting",id:"7-troubleshooting",level:2},{value:"8. Appendix",id:"8-appendix",level:2},{value:"Example Directory Structure",id:"example-directory-structure",level:3},{value:"References",id:"references",level:3}];function a(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"chatbot",children:"Chatbot"})}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsxs)(n.p,{children:["This guide explains how to deploy ",(0,l.jsx)(n.strong,{children:"DeepSeek-R1"})," large language model locally on ",(0,l.jsx)(n.strong,{children:"NVIDIA Jetson Orin"})," devices using ",(0,l.jsx)(n.strong,{children:"Ollama"})," (lightweight inference engine) for offline AI interactions with simple installation."]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"1-overview",children:"1. Overview"}),"\n",(0,l.jsx)(n.p,{children:"LLMs like DeepSeek-R1 are becoming core to edge AI applications. Benefits of running directly on Jetson Orin:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Fully offline operation"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Low-latency responses"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.strong,{children:"Enhanced data privacy"})}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Guide covers:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Environment setup"}),"\n",(0,l.jsx)(n.li,{children:"Ollama installation"}),"\n",(0,l.jsx)(n.li,{children:"Running DeepSeek-R1 model"}),"\n",(0,l.jsx)(n.li,{children:"Optional Open WebUI for web interface"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"2-environment-setup",children:"2. Environment Setup"}),"\n",(0,l.jsx)(n.h3,{id:"hardware",children:"Hardware"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Component"}),(0,l.jsx)(n.th,{children:"Requirement"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Device"}),(0,l.jsx)(n.td,{children:"Jetson Orin (Nano/NX/AGX)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"RAM"}),(0,l.jsx)(n.td,{children:"\u22658GB (more for larger models)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Storage"}),(0,l.jsx)(n.td,{children:"\u226510GB (varies by model size)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"GPU"}),(0,l.jsx)(n.td,{children:"NVIDIA GPU with CUDA support"})]})]})]}),"\n",(0,l.jsx)(n.h3,{id:"software",children:"Software"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Ubuntu 20.04/22.04 (JetPack 5.1.1+ recommended)"}),"\n",(0,l.jsx)(n.li,{children:"NVIDIA CUDA toolkit and drivers (pre-installed in JetPack)"}),"\n",(0,l.jsx)(n.li,{children:"Docker (optional for containerized deployment)"}),"\n"]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:["\u2699\ufe0f Use ",(0,l.jsx)(n.code,{children:"jetson_clocks"})," and check ",(0,l.jsx)(n.code,{children:"nvpmodel"})," to enable max performance mode for best inference."]}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"3-install-ollama-inference-engine",children:"3. Install Ollama (Inference Engine)"}),"\n",(0,l.jsx)(n.h3,{id:"method-a-native-script-installation",children:"Method A: Native Script Installation"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"curl -fsSL https://ollama.com/install.sh | sh\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Installs Ollama service and CLI"}),"\n",(0,l.jsx)(n.li,{children:"Automatically handles dependencies"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"method-b-docker-deployment",children:"Method B: Docker Deployment"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"sudo docker run --runtime=nvidia --rm --network=host \\\r\n  -v ~/ollama:/ollama \\\r\n  -e OLLAMA_MODELS=/ollama \\\r\n  dustynv/ollama:r36.4.0\n"})}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsx)(n.p,{children:"\ud83e\udde9 Docker version maintained by NVIDIA community (dustynv), optimized for Jetson"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"verify-ollama-running",children:"Verify Ollama Running"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ss -tuln | grep 11434\n"})}),"\n",(0,l.jsx)(n.p,{children:"Expected output:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"LISTEN 0 128 127.0.0.1:11434 ...\n"})}),"\n",(0,l.jsxs)(n.p,{children:["Port ",(0,l.jsx)(n.code,{children:"11434"})," listening indicates Ollama service is running."]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"4-run-deepseek-r1-model",children:"4. Run DeepSeek-R1 Model"}),"\n",(0,l.jsx)(n.h3,{id:"launch-model",children:"Launch Model"}),"\n",(0,l.jsx)(n.p,{children:"Run 1.5B parameter version:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ollama run deepseek-r1:1.5b\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Ollama auto-downloads model if not cached"}),"\n",(0,l.jsx)(n.li,{children:"Starts interactive chat in terminal"}),"\n"]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:["\ud83d\udca1 Replace ",(0,l.jsx)(n.code,{children:"1.5b"})," with ",(0,l.jsx)(n.code,{children:"8b"}),", ",(0,l.jsx)(n.code,{children:"14b"})," etc. based on hardware capability"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"model-version-comparison",children:"Model Version Comparison"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Version"}),(0,l.jsx)(n.th,{children:"RAM Needed"}),(0,l.jsx)(n.th,{children:"Notes"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"1.5B"}),(0,l.jsx)(n.td,{children:"~6-8GB"}),(0,l.jsx)(n.td,{children:"For Orin Nano/NX"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"8B+"}),(0,l.jsx)(n.td,{children:"\u226516GB"}),(0,l.jsx)(n.td,{children:"Requires AGX Orin"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"70B"}),(0,l.jsx)(n.td,{children:"\ud83d\udeab"}),(0,l.jsx)(n.td,{children:"Not supported"})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"5-web-interface-open-webui",children:"5. Web Interface (Open WebUI)"}),"\n",(0,l.jsxs)(n.p,{children:["Open WebUI provides browser-based chat interface.\r\n",(0,l.jsx)(n.img,{alt:"open_webui",src:i(72751).A+"",width:"1920",height:"1080"})]}),"\n",(0,l.jsx)(n.h3,{id:"install-open-webui-docker",children:"Install Open WebUI (Docker)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"sudo docker run -d --network=host \\\r\n  -v ${HOME}/open-webui:/app/backend/data \\\r\n  -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\\r\n  --name open-webui \\\r\n  --restart always \\\r\n  ghcr.io/open-webui/open-webui:main\n"})}),"\n",(0,l.jsx)(n.h3,{id:"access-webui",children:"Access WebUI"}),"\n",(0,l.jsx)(n.p,{children:"Browser access:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"http://localhost:3000/\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Graphical interaction with DeepSeek-R1"}),"\n",(0,l.jsx)(n.li,{children:"View chat history and responses"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"6-performance-optimization",children:"6. Performance Optimization"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Optimization"}),(0,l.jsx)(n.th,{children:"Recommendation"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Memory"}),(0,l.jsx)(n.td,{children:"Use smaller models or enable swap"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Jetson Mode"}),(0,l.jsxs)(n.td,{children:["Enable ",(0,l.jsx)(n.code,{children:"MAXN"})," + ",(0,l.jsx)(n.code,{children:"jetson_clocks"})]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Model Cache"}),(0,l.jsxs)(n.td,{children:["Ensure ",(0,l.jsx)(n.code,{children:"~/ollama"})," has space"]})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Monitoring"}),(0,l.jsxs)(n.td,{children:["Use ",(0,l.jsx)(n.code,{children:"htop"}),", ",(0,l.jsx)(n.code,{children:"tegrastats"})]})]})]})]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsx)(n.p,{children:"\ud83d\udcc9 First model load takes ~30-60 sec, faster with cache."}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"7-troubleshooting",children:"7. Troubleshooting"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Issue"}),(0,l.jsx)(n.th,{children:"Solution"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Port 11434 not listening"}),(0,l.jsx)(n.td,{children:"Restart Ollama or check Docker"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Model load failure"}),(0,l.jsx)(n.td,{children:"Insufficient RAM, try smaller version"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Can't access Web UI"}),(0,l.jsx)(n.td,{children:"Check Docker running and network"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"Ollama command not found"}),(0,l.jsxs)(n.td,{children:["Reinstall or add to ",(0,l.jsx)(n.code,{children:"$PATH"})]})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"8-appendix",children:"8. Appendix"}),"\n",(0,l.jsx)(n.h3,{id:"example-directory-structure",children:"Example Directory Structure"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"~/ollama/                # Model cache\r\n~/open-webui/            # WebUI persistent data\n"})}),"\n",(0,l.jsx)(n.h3,{id:"references",children:"References"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/deepseek-ai",children:"DeepSeek-R1 - HuggingFace"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama Docs"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI GitHub"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://forums.developer.nvidia.com/",children:"NVIDIA Jetson Forum"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(a,{...e})}):a(e)}},72751:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/open_webui-d3952c8561c4808c1d447fc061c71174.gif"},28453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>t});var r=i(96540);const l={},s=r.createContext(l);function d(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:d(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);